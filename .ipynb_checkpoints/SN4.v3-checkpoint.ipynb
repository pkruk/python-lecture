{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci Neuronowe - Laboratorium 4. Wybór funkcji kosztu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresja czy klasyfikacja?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Regresja - błąd średniokwadratowy\n",
    "Przyjmijmy, że mamy regresję jednowymiarową (wtedy $f(X_i|\\theta), Y_i \\in \\mathbb{R}$) lub wielowymiarową (wtedy $f(X_i|\\theta), Y_i \\in \\mathbb{R}^m$)\n",
    "$$\n",
    "\\mathcal{L}(\\theta| X, Y) = \\tfrac{1}{N} \\sum_{i=1}^N \\| f(X_i|\\theta) - Y_i \\|_2^2 \n",
    "$$\n",
    "\n",
    "\n",
    "    theanets.Regresor\n",
    "\n",
    "\n",
    "#### Klasyfikacja - błąd entropii krzyżowej (błąd logistyczny)\n",
    "Przyjmijmy że mamy `m` klas i `m` neuronów wyjściowych gora -> wyjscie neuronu maksymalizujemy po wszystkich wyjsciach neuronow wyjsciowych stricte klasyfikacja\n",
    "$$\n",
    "\\mathcal{L}(\\theta| X, Y) = \\tfrac{1}{N} \\sum_{i=1}^N -\\log \\frac{\\exp(f_{Y_i}(X_i|\\theta))}{\\sum_{k=1}^m \\exp(f_k(X_i|\\theta))}\n",
    "$$\n",
    "\n",
    "    theanets.Classifer\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytania do dyskusji:\n",
    "\n",
    "* Dlaczego te dwie funkcje kosztu są najbardziej popularne?\n",
    "* Czy patrząc na nie przychodzi wam na myśl jakaś prosta inna funkcja kosztu? Jaka? Jakie miałaby cechy?\n",
    "* Jaka jest różnica pomiędzy tymi kosztami? W ogólności? A dla problemu binarnego?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadania\n",
    "\n",
    "Stwórz trzy architektury i przetestuj je w problemie rozpoznawania pisma:\n",
    "\n",
    "* Model regresji z jednym neuronem wyjściowem\n",
    "* Model regresji z `m` neuronami wyjściowymi (kodowanie klas one-hot-encoding)\n",
    "* Model klasyfikacji  z `m` neuronami wyjściowymi (kodowanie klas one-hot-encoding)\n",
    "\n",
    "1) dokonczyc\n",
    "2) przyjrzyj sie projektowi 2 i koncowemu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza procesu uczenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przy użyciu `theanets` oraz naszego zbioru danych (możesz zredukować go do mniejszej liczby przykładów żeby zwiększyć szybkość działania) przeprowadź eksperyment, w którym po każdej iteracji procesu uczenia sieci neuronowej sprawdzasz jaki wynik osiąga ona na zbiorze uczącym oraz jaki na zbiorze testującym. Zaprezentuj wynik tego eksperymentu w postaci wykresów (zależności czasu od accuracy). Najlepiej twórz wykres co określoną liczbę iteracji żeby powstawał on na bieżąco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400) (5000, 1)\n",
      "0\n",
      "0 error on training set:  0.6696\n",
      "0 error on testing set:  0.6752\n",
      "1\n",
      "1 error on training set:  0.6880\n",
      "1 error on testing set:  0.6772\n",
      "2\n",
      "2 error on training set:  0.6204\n",
      "2 error on testing set:  0.6020\n",
      "3\n",
      "3 error on training set:  0.6020\n",
      "3 error on testing set:  0.6252\n",
      "4\n",
      "4 error on training set:  0.6152\n",
      "4 error on testing set:  0.5944\n",
      "5\n",
      "5 error on training set:  0.6008\n",
      "5 error on testing set:  0.5928\n",
      "6\n",
      "6 error on training set:  0.7524\n",
      "6 error on testing set:  0.7332\n",
      "7\n",
      "7 error on training set:  0.4956\n",
      "7 error on testing set:  0.5120\n",
      "8\n",
      "8 error on training set:  0.4804\n",
      "8 error on testing set:  0.4908\n",
      "9\n",
      "9 error on training set:  0.4716\n",
      "9 error on testing set:  0.4892\n",
      "10\n",
      "10 error on training set:  0.4664\n",
      "10 error on testing set:  0.4920\n",
      "11\n",
      "11 error on training set:  0.8208\n",
      "11 error on testing set:  0.8092\n",
      "12\n",
      "12 error on training set:  0.5052\n",
      "12 error on testing set:  0.5244\n",
      "13\n",
      "13 error on training set:  0.7844\n",
      "13 error on testing set:  0.7744\n",
      "14\n",
      "14 error on training set:  0.5224\n",
      "14 error on testing set:  0.5408\n",
      "15\n",
      "15 error on training set:  0.7736\n",
      "15 error on testing set:  0.7656\n",
      "16\n",
      "16 error on training set:  0.8388\n",
      "16 error on testing set:  0.8352\n",
      "17\n",
      "17 error on training set:  0.8204\n",
      "17 error on testing set:  0.8032\n",
      "18\n",
      "18 error on training set:  0.4148\n",
      "18 error on testing set:  0.4640\n",
      "19\n",
      "19 error on training set:  0.4508\n",
      "19 error on testing set:  0.4948\n",
      "20\n",
      "20 error on training set:  0.5380\n",
      "20 error on testing set:  0.5728\n",
      "21\n",
      "21 error on training set:  0.4784\n",
      "21 error on testing set:  0.5204\n",
      "22\n",
      "22 error on training set:  0.8728\n",
      "22 error on testing set:  0.8476\n",
      "23\n",
      "23 error on training set:  0.8516\n",
      "23 error on testing set:  0.8332\n",
      "24\n",
      "24 error on training set:  0.3456\n",
      "24 error on testing set:  0.4252\n",
      "25\n",
      "25 error on training set:  0.4148\n",
      "25 error on testing set:  0.4712\n",
      "26\n",
      "26 error on training set:  0.3516\n",
      "26 error on testing set:  0.4284\n",
      "27\n",
      "27 error on training set:  0.7716\n",
      "27 error on testing set:  0.7828\n",
      "28\n",
      "28 error on training set:  0.8088\n",
      "28 error on testing set:  0.8024\n",
      "29\n",
      "29 error on training set:  0.6820\n",
      "29 error on testing set:  0.6904\n",
      "30\n",
      "30 error on training set:  0.3652\n",
      "30 error on testing set:  0.4328\n",
      "31\n",
      "31 error on training set:  0.3668\n",
      "31 error on testing set:  0.4336\n",
      "32\n",
      "32 error on training set:  0.8516\n",
      "32 error on testing set:  0.8368\n",
      "33\n",
      "33 error on training set:  0.4796\n",
      "33 error on testing set:  0.5172\n",
      "34\n",
      "34 error on training set:  0.6796\n",
      "34 error on testing set:  0.7252\n",
      "35\n",
      "35 error on training set:  0.4424\n",
      "35 error on testing set:  0.4976\n",
      "36\n",
      "36 error on training set:  0.4192\n",
      "36 error on testing set:  0.4816\n",
      "37\n",
      "37 error on training set:  0.6864\n",
      "37 error on testing set:  0.7196\n",
      "38\n",
      "38 error on training set:  0.3636\n",
      "38 error on testing set:  0.4380\n",
      "39\n",
      "39 error on training set:  0.3140\n",
      "39 error on testing set:  0.4128\n",
      "40\n",
      "40 error on training set:  0.3296\n",
      "40 error on testing set:  0.4308\n",
      "41\n",
      "41 error on training set:  0.6060\n",
      "41 error on testing set:  0.6512\n",
      "42\n",
      "42 error on training set:  0.8204\n",
      "42 error on testing set:  0.8116\n",
      "43\n",
      "43 error on training set:  0.8128\n",
      "43 error on testing set:  0.8100\n",
      "44\n",
      "44 error on training set:  0.3784\n",
      "44 error on testing set:  0.4652\n",
      "45\n",
      "45 error on training set:  0.5400\n",
      "45 error on testing set:  0.5856\n",
      "46\n",
      "46 error on training set:  0.5832\n",
      "46 error on testing set:  0.6424\n",
      "47\n",
      "47 error on training set:  0.6588\n",
      "47 error on testing set:  0.7012\n",
      "48\n",
      "48 error on training set:  0.2904\n",
      "48 error on testing set:  0.4036\n",
      "49\n",
      "49 error on training set:  0.7812\n",
      "49 error on testing set:  0.7960\n",
      "50\n",
      "50 error on training set:  0.3228\n",
      "50 error on testing set:  0.4260\n",
      "51\n",
      "51 error on training set:  0.3404\n",
      "51 error on testing set:  0.4340\n",
      "52\n",
      "52 error on training set:  0.3672\n",
      "52 error on testing set:  0.4572\n",
      "53\n",
      "53 error on training set:  0.3412\n",
      "53 error on testing set:  0.4420\n",
      "54\n",
      "54 error on training set:  0.5528\n",
      "54 error on testing set:  0.6216\n",
      "55\n",
      "55 error on training set:  0.3772\n",
      "55 error on testing set:  0.4720\n",
      "56\n",
      "56 error on training set:  0.4000\n",
      "56 error on testing set:  0.4776\n",
      "57\n",
      "57 error on training set:  0.2896\n",
      "57 error on testing set:  0.4104\n",
      "58\n",
      "58 error on training set:  0.3840\n",
      "58 error on testing set:  0.4680\n",
      "59\n",
      "59 error on training set:  0.7672\n",
      "59 error on testing set:  0.7856\n",
      "60\n",
      "60 error on training set:  0.2960\n",
      "60 error on testing set:  0.4172\n",
      "61\n",
      "61 error on training set:  0.5196\n",
      "61 error on testing set:  0.5964\n",
      "62\n",
      "62 error on training set:  0.4152\n",
      "62 error on testing set:  0.5236\n",
      "63\n",
      "63 error on training set:  0.6592\n",
      "63 error on testing set:  0.7036\n",
      "64\n",
      "64 error on training set:  0.3764\n",
      "64 error on testing set:  0.4812\n",
      "65\n",
      "65 error on training set:  0.2984\n",
      "65 error on testing set:  0.4204\n",
      "66\n",
      "66 error on training set:  0.6028\n",
      "66 error on testing set:  0.6536\n",
      "67\n",
      "67 error on training set:  0.8304\n",
      "67 error on testing set:  0.8264\n",
      "68\n",
      "68 error on training set:  0.6084\n",
      "68 error on testing set:  0.6716\n",
      "69\n",
      "69 error on training set:  0.3012\n",
      "69 error on testing set:  0.4104\n",
      "70\n",
      "70 error on training set:  0.3208\n",
      "70 error on testing set:  0.4420\n",
      "71\n",
      "71 error on training set:  0.7268\n",
      "71 error on testing set:  0.7532\n",
      "72\n",
      "72 error on training set:  0.6592\n",
      "72 error on testing set:  0.7032\n",
      "73\n",
      "73 error on training set:  0.6072\n",
      "73 error on testing set:  0.6668\n",
      "74\n",
      "74 error on training set:  0.6088\n",
      "74 error on testing set:  0.6744\n",
      "75\n",
      "75 error on training set:  0.5384\n",
      "75 error on testing set:  0.6052\n",
      "76\n",
      "76 error on training set:  0.3108\n",
      "76 error on testing set:  0.4204\n",
      "77\n",
      "77 error on training set:  0.6336\n",
      "77 error on testing set:  0.7096\n",
      "78\n",
      "78 error on training set:  0.7688\n",
      "78 error on testing set:  0.7960\n",
      "79\n",
      "79 error on training set:  0.5832\n",
      "79 error on testing set:  0.6516\n",
      "80\n",
      "80 error on training set:  0.8204\n",
      "80 error on testing set:  0.8120\n",
      "81\n",
      "81 error on training set:  0.7480\n",
      "81 error on testing set:  0.7776\n",
      "82\n",
      "82 error on training set:  0.3972\n",
      "82 error on testing set:  0.5152\n",
      "83\n",
      "83 error on training set:  0.8064\n",
      "83 error on testing set:  0.8008\n",
      "84\n",
      "84 error on training set:  0.4212\n",
      "84 error on testing set:  0.5240\n",
      "85\n",
      "85 error on training set:  0.3852\n",
      "85 error on testing set:  0.4908\n",
      "86\n",
      "86 error on training set:  0.5516\n",
      "86 error on testing set:  0.5920\n",
      "87\n",
      "87 error on training set:  0.2804\n",
      "87 error on testing set:  0.4088\n",
      "88\n",
      "88 error on training set:  0.3128\n",
      "88 error on testing set:  0.4468\n",
      "89\n",
      "89 error on training set:  0.8100\n",
      "89 error on testing set:  0.8116\n",
      "90\n",
      "90 error on training set:  0.3208\n",
      "90 error on testing set:  0.4472\n",
      "91\n",
      "91 error on training set:  0.7792\n",
      "91 error on testing set:  0.7888\n",
      "92\n",
      "92 error on training set:  0.6372\n",
      "92 error on testing set:  0.6960\n",
      "93\n",
      "93 error on training set:  0.2764\n",
      "93 error on testing set:  0.3988\n",
      "94\n",
      "94 error on training set:  0.2816\n",
      "94 error on testing set:  0.4160\n",
      "95\n",
      "95 error on training set:  0.7464\n",
      "95 error on testing set:  0.7568\n",
      "96\n",
      "96 error on training set:  0.5520\n",
      "96 error on testing set:  0.6396\n",
      "97\n",
      "97 error on training set:  0.6020\n",
      "97 error on testing set:  0.6840\n",
      "98\n",
      "98 error on training set:  0.4656\n",
      "98 error on testing set:  0.5620\n",
      "99\n",
      "99 error on training set:  0.5628\n",
      "99 error on testing set:  0.6412\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import theanets\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "D = loadmat('data/ex3data1.mat')\n",
    "X, y = D['X'], D['y']\n",
    "\n",
    "\n",
    "#y=y.ravel() # y musi być wektorem, a nie macierzą (5000, 1)\n",
    "#y = y.reshape(y.shape[0],1)\n",
    "y[y == 10] = 0\n",
    "print X.shape, y.shape\n",
    "\n",
    "a_train, a_test, b_train, b_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "x = X[0]\n",
    "#b = b_train.ravel()\n",
    "#print a_train.shape[1]\n",
    "exp = theanets.Experiment(\n",
    "#    theanets.recurrent.Regressor,\n",
    "#     layers=(a_train.shape[1], \n",
    "#             10, \n",
    "#             10)\n",
    "#     )\n",
    "theanets.feedforward.Regressor,\n",
    "                    layers=(400,10,1),\n",
    "                    optimize='sgd',\n",
    "                    activation='tanh')\n",
    "\n",
    "d = []\n",
    "for iters, (train, valid) in enumerate(exp.itertrain((a_train, b_train), optimize='sgd', \n",
    "                                                    learning_rate=0.1, momentum=0.1,\n",
    "                                                    patience=1)):\n",
    "        print iters\n",
    "        tr = 1.0 - accuracy_score(b_train, exp.network.predict(a_train).astype(int), normalize=True)\n",
    "        te = 1.0 - accuracy_score(b_test, exp.network.predict(a_test).astype(int), normalize=True)\n",
    "        print iters, \"error on training set: %7.4f\" % tr\n",
    "        print iters, \"error on testing set: %7.4f\" % te\n",
    "        plt.figure()\n",
    "        d.append(tr)\n",
    "        plt.plot(d)\n",
    "        plt.savefig('wyk.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytania analityczne / do dyskusji\n",
    "\n",
    "* Co się dzieje gdy zmienisz wielkość warstwy ukrytej na np. `10`? A jeśli na `2000`? \n",
    "* Jak wyglądają te wykresy dla modeli regresji i klasyfikacji? \n",
    "* Skąd takie zależności? \n",
    "* Czy potrafisz wydzielić jakieś sekcje w obserwowanych wykresach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "216 error on training set:  0.0312\n",
    "216 error on testing set:  0.0940 neurony 10 w warstwie ukrytej\n",
    "warstwa 200, strasznie dlugo liczy i jakby szybciej zbiega przy 60 iteracji mam juz 0.08 i  0.10 i na trenujacym wiekszy\n",
    "blad, ale mniejsza roznica wzgledem testujacego:\n",
    "216 error on training set:  0.0312\n",
    "216 error on testing set:  0.0896\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe rezultaty\n",
    "\n",
    "\n",
    "#### Regresja 10 neuronów\n",
    "\n",
    "<img src=\"files/SN4_img/10reg.png\" width=\"600\">\n",
    "\n",
    "#### Klasyfikacja 10 neuronów\n",
    "\n",
    "<img src=\"files/SN4_img/10cla.png\" width=\"600\">\n",
    "\n",
    "#### Regresja 100 neuronów\n",
    "\n",
    "<img src=\"files/SN4_img/100reg.png\" width=\"600\">\n",
    "\n",
    "#### Klasyfikacja 100 neuronów\n",
    "\n",
    "<img src=\"files/SN4_img/100cla.png\" width=\"600\">\n",
    "\n",
    "#### Regresja 1000 neuronów\n",
    "\n",
    "<img src=\"files/SN4_img/1000reg.png\" width=\"600\">\n",
    "\n",
    "#### Klasyfikacja 1000 neuronów\n",
    "\n",
    "<img src=\"files/SN4_img/1000cla.png\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
