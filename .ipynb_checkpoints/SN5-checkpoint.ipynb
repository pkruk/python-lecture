{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci Neuronowe - Laboratorium 5. Regularyzacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularyzacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Regresja - błąd średniokwadratowy\n",
    "Przyjmijmy, że mamy regresję jednowymiarową (wtedy $f(X_i|\\theta), Y_i \\in \\mathbb{R}$) lub wielowymiarową (wtedy $f(X_i), Y_i \\in \\mathbb{R}^m$)\n",
    "$$\n",
    "\\mathcal{L}(\\theta| X, Y) = \\tfrac{1}{N} \\sum_{i=1}^N \\| f(X_i|\\theta) - Y_i \\|_2^2 + \\Omega(\\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "    theanets.Regresor\n",
    "\n",
    "##### Regularyzacja L$^2$\n",
    "$$\n",
    "\\mathcal{L}(\\theta| X, Y) = \\tfrac{1}{N} \\sum_{i=1}^N \\| f(X_i|\\theta) - Y_i \\|_2^2 + \\tfrac{\\alpha}{2}\\|\\theta\\|_2^2\n",
    "$$\n",
    "\n",
    "##### Regularyzacja L$^1$\n",
    "$$\n",
    "\\mathcal{L}(\\theta| X, Y) = \\tfrac{1}{N} \\sum_{i=1}^N \\| f(X_i|\\theta) - Y_i \\|_2^2 + \\alpha\\|\\theta\\|_1\n",
    "$$\n",
    "\n",
    "    theanets.Regressor, weight_l2=..., weight_l1=...\n",
    "\n",
    "#### Klasyfikacja - błąd entropii krzyżowej (błąd logistyczny)\n",
    "Przyjmijmy że mamy `m` klas i `m` neuronów wyjściowych\n",
    "$$\n",
    "\\mathcal{L}(\\theta| X, Y) = \\tfrac{1}{N} \\sum_{i=1}^N -\\log \\frac{\\exp(f_{Y_i}(X_i|\\theta))}{\\sum_{k=1}^m \\exp(f_k(X_i|\\theta))} + \\Omega(\\theta)\n",
    "$$\n",
    "\n",
    "    theanets.Classifer\n",
    "\n",
    "##### Regularyzacja L$^2$\n",
    "$$\n",
    "\\mathcal{L}(\\theta| X, Y) = \\tfrac{1}{N} \\sum_{i=1}^N -\\log \\frac{\\exp(f_{Y_i}(X_i|\\theta))}{\\sum_{k=1}^m \\exp(f_k(X_i|\\theta))} + \\tfrac{\\alpha}{2}\\|\\theta\\|_2^2\n",
    "$$\n",
    "\n",
    "##### Regularyzacja L$^1$\n",
    "$$\n",
    "\\mathcal{L}(\\theta| X, Y) = \\tfrac{1}{N} \\sum_{i=1}^N -\\log \\frac{\\exp(f_{Y_i}(X_i|\\theta))}{\\sum_{k=1}^m \\exp(f_k(X_i|\\theta))} + \\alpha\\|\\theta\\|_1\n",
    "$$    \n",
    "\n",
    "    theanets.Classifier, weight_l2=..., weight_l1=..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytania do dyskusji:\n",
    "\n",
    "* Dlaczego te dwie funkcje regularyzacji są najbardziej popularne?\n",
    "* Czy patrząc na nie przychodzi wam na myśl jakaś prosta inna funkcja regularyzacji? Jaka? Jakie miałaby cechy?\n",
    "* Jaka jest różnica pomiędzy tymi regularyzacjami?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadania\n",
    "\n",
    "Załaduj zbiór z projektu 1, podziel go w proporcjach 3:1 na zbiór trenujący i testujący. Zbuduj sieć neuronową z jedną warstwą ukrytą (np. 1000 neuronów) i spróbuj ją nauczyć.\n",
    "\n",
    "* Czy sieć się rozsądnie uczy? Dlaczego? Jak sobie z tym poradzić?\n",
    "* Poeksperymentuj z różnymi wartościami funkcji regularyzacji, czy potrafisz osiągnąć wynik ~79%?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import normalize\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X, y = load_svmlight_file('data/task1.train')\n",
    "X = normalize(X)\n",
    "X = X.toarray()\n",
    "y = y.reshape(y.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "a_train, a_test, b_train, b_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(647, 1850) (319, 1850) (647, 1) (319, 1)\n",
      "(966, 1850) (966, 1)\n"
     ]
    }
   ],
   "source": [
    "print a_train.shape, a_test.shape, b_train.shape, b_test.shape\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print type(X[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0 error on training set:  1.0000\n",
      "0 error on testing set:  1.0000\n",
      "1\n",
      "1 error on training set:  1.0000\n",
      "1 error on testing set:  1.0000\n",
      "2\n",
      "2 error on training set:  1.0000\n",
      "2 error on testing set:  1.0000\n",
      "3\n",
      "3 error on training set:  1.0000\n",
      "3 error on testing set:  1.0000\n",
      "4\n",
      "4 error on training set:  1.0000\n",
      "4 error on testing set:  1.0000\n",
      "5\n",
      "5 error on training set:  1.0000\n",
      "5 error on testing set:  1.0000\n"
     ]
    }
   ],
   "source": [
    "import theanets\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exp = theanets.Experiment(theanets.feedforward.Regressor,\n",
    "                    layers=(a_train.shape[1],1000,1),\n",
    "                    optimize='sgd',\n",
    "                    activation='tanh')\n",
    "\n",
    "d = []\n",
    "for iters, (train, valid) in enumerate(exp.itertrain((a_train, b_train), optimize='sgd', \n",
    "                                                    learning_rate=0.1, momentum=0.1,hidden_l2=0.5,\n",
    "                                                    patience=1)):\n",
    "        print iters\n",
    "        tr = 1.0 - accuracy_score(b_train, exp.network.predict(a_train).astype(int), normalize=True)\n",
    "        te = 1.0 - accuracy_score(b_test, exp.network.predict(a_test).astype(int), normalize=True)\n",
    "        print iters, \"error on training set: %7.4f\" % tr\n",
    "        print iters, \"error on testing set: %7.4f\" % te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1000 w ukrytej\n",
    "135 error on training set:  0.5966\n",
    "135 error on testing set:  0.6176\n",
    "136\n",
    "136 error on training set:  0.5920\n",
    "136 error on testing set:  0.6082\n",
    "137\n",
    "137 error on training set:  0.8068\n",
    "137 error on testing set:  0.8088\n",
    "138\n",
    "138 error on training set:  0.5750\n",
    "138 error on testing set:  0.6332\n",
    "139\n",
    "139 error on training set:  0.7311\n",
    "139 error on testing set:  0.7524\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "przy regularyzacji \n",
    "hidden_l1=0.1, patience=1)):\n",
    "WTF???\n",
    "0 error on training set:  1.0000\n",
    "0 error on testing set:  1.0000\n",
    "1\n",
    "1 error on training set:  1.0000\n",
    "1 error on testing set:  1.0000\n",
    "2\n",
    "2 error on training set:  1.0000\n",
    "2 error on testing set:  1.0000\n",
    "3\n",
    "3 error on training set:  1.0000\n",
    "3 error on testing set:  1.0000\n",
    "4\n",
    "4 error on training set:  1.0000\n",
    "4 error on testing set:  1.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_l1=0.1,hidden_l2=0.5,\n",
    " error on training set:  1.0000\n",
    "0 error on testing set:  1.0000\n",
    "1\n",
    "1 error on training set:  1.0000\n",
    "1 error on testing set:  1.0000\n",
    "2\n",
    "2 error on training set:  1.0000\n",
    "2 error on testing set:  1.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_l2=0.5,\n",
    "0\n",
    "0 error on training set:  1.0000\n",
    "0 error on testing set:  1.0000\n",
    "1\n",
    "1 error on training set:  1.0000\n",
    "1 error on testing set:  1.0000\n",
    "2\n",
    "2 error on training set:  1.0000\n",
    "2 error on testing set:  1.0000\n",
    "3\n",
    "3 error on training set:  1.0000\n",
    "3 error on testing set:  1.0000\n",
    "4\n",
    "4 error on training set:  1.0000\n",
    "4 error on testing set:  1.0000\n",
    "5\n",
    "5 error on training set:  1.0000\n",
    "5 error on testing set:  1.0000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
